{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnamese ABSA Dataset Preprocessing (Fixed)\n",
    "\n",
    "This notebook fetches raw VLSP 2018 ABSA data from GitHub and applies comprehensive Vietnamese preprocessing.\n",
    "\n",
    "**Data Source:** https://github.com/ancan203/SentimentAnalysis/VLSP2018/\n",
    "\n",
    "**Features:**\n",
    "- Downloads raw VLSP2018 files directly from GitHub\n",
    "- Vietnamese text cleaning (HTML, emoji, URL removal)\n",
    "- Vietnamese tone normalization (VinAI rules)\n",
    "- Traditional preprocessing (lowercase, whitespace, short words)\n",
    "- Outputs compatible with both Traditional ML and PhoBERT notebooks\n",
    "- Proper train/dev/test splits for fair comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji regex pandas numpy\n",
    "\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Raw VLSP2018 Data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw VLSP2018 data files from GitHub\n",
    "base_url = \"https://raw.githubusercontent.com/ancan203/SentimentAnalysis/main/VLSP2018/\"\n",
    "\n",
    "# Raw data files to download\n",
    "raw_files = [\n",
    "    'VLSP2018-SA-Hotel-train.txt',\n",
    "    'VLSP2018-SA-Hotel-dev.txt', \n",
    "    'VLSP2018-SA-Hotel-test.txt',\n",
    "    'VLSP2018-SA-Restaurant-train.txt',\n",
    "    'VLSP2018-SA-Restaurant-dev.txt',\n",
    "    'VLSP2018-SA-Restaurant-test.txt'\n",
    "]\n",
    "\n",
    "# Create raw_data directory\n",
    "os.makedirs('raw_data', exist_ok=True)\n",
    "\n",
    "print(\"Downloading raw VLSP2018 ABSA data files...\")\n",
    "\n",
    "# Download files\n",
    "for filename in raw_files:\n",
    "    try:\n",
    "        url = base_url + filename\n",
    "        local_path = f'raw_data/{filename}'\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "        print(f\"✓ Downloaded {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to download {filename}: {e}\")\n",
    "\n",
    "print(\"\\nRaw data download complete!\")\n",
    "print(\"Files available for preprocessing:\")\n",
    "for file in raw_files:\n",
    "    if os.path.exists(f'raw_data/{file}'):\n",
    "        size = os.path.getsize(f'raw_data/{file}')\n",
    "        print(f\"  {file}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vietnamese Text Cleaning Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseTextCleaner:\n",
    "    \"\"\"Vietnamese-specific text cleaning utilities\"\"\"\n",
    "    \n",
    "    VN_CHARS = 'áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ'\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_html(text):\n",
    "        \"\"\"Remove HTML tags\"\"\"\n",
    "        return re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_emoji(text):\n",
    "        \"\"\"Remove emojis\"\"\"\n",
    "        return emoji.replace_emoji(text, '')\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_url(text):\n",
    "        \"\"\"Remove URLs\"\"\"\n",
    "        return re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_email(text):\n",
    "        \"\"\"Remove email addresses\"\"\"\n",
    "        return re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_phone_number(text):\n",
    "        \"\"\"Remove phone numbers\"\"\"\n",
    "        return re.sub(r'\\b\\d{3,4}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_hashtags(text):\n",
    "        \"\"\"Remove hashtags\"\"\"\n",
    "        return re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_unnecessary_characters(text):\n",
    "        \"\"\"Remove non-essential characters, keep Vietnamese characters\"\"\"\n",
    "        # Keep letters, numbers, Vietnamese characters, spaces, and basic punctuation\n",
    "        pattern = r'[^a-zA-Z0-9\\s' + re.escape(VietnameseTextCleaner.VN_CHARS) + r'.,!?;:-]'\n",
    "        return re.sub(pattern, '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        \"\"\"Apply all Vietnamese cleaning steps\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return ''\n",
    "        \n",
    "        # Apply cleaning steps\n",
    "        text = VietnameseTextCleaner.remove_html(text)\n",
    "        text = VietnameseTextCleaner.remove_emoji(text)\n",
    "        text = VietnameseTextCleaner.remove_url(text)\n",
    "        text = VietnameseTextCleaner.remove_email(text)\n",
    "        text = VietnameseTextCleaner.remove_phone_number(text)\n",
    "        text = VietnameseTextCleaner.remove_hashtags(text)\n",
    "        text = VietnameseTextCleaner.remove_unnecessary_characters(text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "print(\"Vietnamese text cleaner loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseToneNormalizer:\n",
    "    \"\"\"Vietnamese tone normalization utilities\"\"\"\n",
    "    \n",
    "    VOWELS_TABLE = [\n",
    "        ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "        ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "        ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "        ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
    "        ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "        ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
    "        ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
    "        ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "        ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "        ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
    "        ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "        ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']\n",
    "    ]\n",
    "    \n",
    "    VOWELS_TO_IDS = {\n",
    "        'a': (0, 0), 'à': (0, 1), 'á': (0, 2), 'ả': (0, 3), 'ã': (0, 4), 'ạ': (0, 5), \n",
    "        'ă': (1, 0), 'ằ': (1, 1), 'ắ': (1, 2), 'ẳ': (1, 3), 'ẵ': (1, 4), 'ặ': (1, 5), \n",
    "        'â': (2, 0), 'ầ': (2, 1), 'ấ': (2, 2), 'ẩ': (2, 3), 'ẫ': (2, 4), 'ậ': (2, 5), \n",
    "        'e': (3, 0), 'è': (3, 1), 'é': (3, 2), 'ẻ': (3, 3), 'ẽ': (3, 4), 'ẹ': (3, 5), \n",
    "        'ê': (4, 0), 'ề': (4, 1), 'ế': (4, 2), 'ể': (4, 3), 'ễ': (4, 4), 'ệ': (4, 5), \n",
    "        'i': (5, 0), 'ì': (5, 1), 'í': (5, 2), 'ỉ': (5, 3), 'ĩ': (5, 4), 'ị': (5, 5), \n",
    "        'o': (6, 0), 'ò': (6, 1), 'ó': (6, 2), 'ỏ': (6, 3), 'õ': (6, 4), 'ọ': (6, 5), \n",
    "        'ô': (7, 0), 'ồ': (7, 1), 'ố': (7, 2), 'ổ': (7, 3), 'ỗ': (7, 4), 'ộ': (7, 5), \n",
    "        'ơ': (8, 0), 'ờ': (8, 1), 'ớ': (8, 2), 'ở': (8, 3), 'ỡ': (8, 4), 'ợ': (8, 5), \n",
    "        'u': (9, 0), 'ù': (9, 1), 'ú': (9, 2), 'ủ': (9, 3), 'ũ': (9, 4), 'ụ': (9, 5), \n",
    "        'ư': (10, 0), 'ừ': (10, 1), 'ứ': (10, 2), 'ử': (10, 3), 'ữ': (10, 4), 'ự': (10, 5), \n",
    "        'y': (11, 0), 'ỳ': (11, 1), 'ý': (11, 2), 'ỷ': (11, 3), 'ỹ': (11, 4), 'ỵ': (11, 5)\n",
    "    }\n",
    "    \n",
    "    VINAI_NORMALIZED_TONE = {\n",
    "        'òa': 'oà', 'Òa': 'Oà', 'ÒA': 'OÀ', \n",
    "        'óa': 'oá', 'Óa': 'Oá', 'ÓA': 'OÁ', \n",
    "        'ỏa': 'oả', 'Ỏa': 'Oả', 'ỎA': 'OẢ',\n",
    "        'õa': 'oã', 'Õa': 'Oã', 'ÕA': 'OÃ',\n",
    "        'ọa': 'oạ', 'Ọa': 'Oạ', 'ỌA': 'OẠ',\n",
    "        'òe': 'oè', 'Òe': 'Oè', 'ÒE': 'OÈ',\n",
    "        'óe': 'oé', 'Óe': 'Oé', 'ÓE': 'OÉ',\n",
    "        'ỏe': 'oẻ', 'Ỏe': 'Oẻ', 'ỎE': 'OẺ',\n",
    "        'õe': 'oẽ', 'Õe': 'Oẽ', 'ÕE': 'OẼ',\n",
    "        'ọe': 'oẹ', 'Ọe': 'Oẹ', 'ỌE': 'OẸ',\n",
    "        'ùy': 'uỳ', 'Ùy': 'Uỳ', 'ÙY': 'UỲ',\n",
    "        'úy': 'uý', 'Úy': 'Uý', 'ÚY': 'UÝ',\n",
    "        'ủy': 'uỷ', 'Ủy': 'Uỷ', 'ỦY': 'UỶ',\n",
    "        'ũy': 'uỹ', 'Ũy': 'Uỹ', 'ŨY': 'UỸ',\n",
    "        'ụy': 'uỵ', 'Ụy': 'Uỵ', 'ỤY': 'UỴ',\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_unicode(text):\n",
    "        \"\"\"Normalize unicode characters\"\"\"\n",
    "        import unicodedata\n",
    "        return unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_sentence_typing(text, vinai_normalization=True):\n",
    "        \"\"\"Normalize Vietnamese typing patterns\"\"\"\n",
    "        if vinai_normalization:\n",
    "            for wrong, correct in VietnameseToneNormalizer.VINAI_NORMALIZED_TONE.items():\n",
    "                text = text.replace(wrong, correct)\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_word_typing(word):\n",
    "        \"\"\"Normalize Vietnamese word typing\"\"\"\n",
    "        # Simplified normalization - can be expanded\n",
    "        return word\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_vietnamese_word(word):\n",
    "        \"\"\"Check if word is valid Vietnamese\"\"\"\n",
    "        # Simplified check - contains Vietnamese characters\n",
    "        return any(c in VietnameseTextCleaner.VN_CHARS for c in word)\n",
    "\n",
    "print(\"Vietnamese tone normalizer loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraditionalPreprocessor:\n",
    "    \"\"\"Traditional text preprocessing utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_lowercase(text):\n",
    "        \"\"\"Convert to lowercase\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_punctuation(text):\n",
    "        \"\"\"Remove punctuation (except Vietnamese characters)\"\"\"\n",
    "        # Keep basic punctuation that might be meaningful\n",
    "        return re.sub(r'[^\\w\\s' + re.escape(VietnameseTextCleaner.VN_CHARS) + r'.,!?;:-]', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_extra_whitespace(text):\n",
    "        \"\"\"Remove extra whitespace\"\"\"\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_numbers(text):\n",
    "        \"\"\"Remove standalone numbers\"\"\"\n",
    "        return re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_words(text, min_length=2):\n",
    "        \"\"\"Remove words shorter than min_length\"\"\"\n",
    "        words = text.split()\n",
    "        return ' '.join([word for word in words if len(word) >= min_length])\n",
    "\n",
    "print(\"Traditional preprocessor loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main VLSP2018 Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLSP2018Preprocessor:\n",
    "    \"\"\"Main preprocessor for VLSP 2018 ABSA dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, apply_vietnamese_steps=True, apply_tone_normalization=True):\n",
    "        self.apply_vietnamese_steps = apply_vietnamese_steps\n",
    "        self.apply_tone_normalization = apply_tone_normalization\n",
    "        self.stats = {\n",
    "            'processed_samples': 0,\n",
    "            'skipped_samples': 0,\n",
    "            'domains': {},\n",
    "            'aspects': {},\n",
    "            'sentiments': {}\n",
    "        }\n",
    "    \n",
    "    def parse_vlsp_file(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Parse VLSP 2018 format file\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            \n",
    "        # Split into individual samples\n",
    "        raw_samples = content.split('\\n\\n')\n",
    "        \n",
    "        for i, raw_sample in enumerate(raw_samples):\n",
    "            if raw_sample.strip():\n",
    "                try:\n",
    "                    sample = self._parse_single_sample(raw_sample.strip(), i+1)\n",
    "                    if sample:\n",
    "                        samples.append(sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing sample {i+1}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _parse_single_sample(self, raw_sample: str, sample_id: int) -> Dict:\n",
    "        \"\"\"Parse a single sample from VLSP format\"\"\"\n",
    "        lines = raw_sample.strip().split('\\n')\n",
    "        \n",
    "        if len(lines) < 2:\n",
    "            return None\n",
    "        \n",
    "        # First line is the text\n",
    "        text = lines[0].strip()\n",
    "        \n",
    "        # Remaining lines are aspect-sentiment pairs\n",
    "        aspect_sentiments = []\n",
    "        \n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if line and '{' in line:\n",
    "                # Parse format: {aspect_category}#{aspect_term}#sentiment\n",
    "                # Or: {aspect_category}#sentiment\n",
    "                line = line.strip('{}').strip()\n",
    "                parts = line.split('#')\n",
    "                \n",
    "                if len(parts) >= 2:\n",
    "                    if len(parts) == 2:\n",
    "                        aspect = parts[0].strip()\n",
    "                        sentiment = parts[1].strip()\n",
    "                    else:\n",
    "                        # Combine category and term\n",
    "                        aspect = f\"{parts[0].strip()}#{parts[1].strip()}\"\n",
    "                        sentiment = parts[2].strip()\n",
    "                    \n",
    "                    aspect_sentiments.append({\n",
    "                        'aspect': aspect,\n",
    "                        'sentiment': sentiment\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'sample_id': sample_id,\n",
    "            'text': text,\n",
    "            'aspect_sentiments': aspect_sentiments\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Apply all preprocessing steps to text\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return ''\n",
    "        \n",
    "        # Vietnamese-specific steps\n",
    "        if self.apply_vietnamese_steps:\n",
    "            text = VietnameseTextCleaner.process_text(text)\n",
    "            \n",
    "            if self.apply_tone_normalization:\n",
    "                text = VietnameseToneNormalizer.normalize_unicode(text)\n",
    "                text = VietnameseToneNormalizer.normalize_sentence_typing(text)\n",
    "        \n",
    "        # Traditional preprocessing steps\n",
    "        text = TraditionalPreprocessor.to_lowercase(text)\n",
    "        text = TraditionalPreprocessor.remove_extra_whitespace(text)\n",
    "        text = TraditionalPreprocessor.remove_short_words(text, min_length=2)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def process_dataset(self, samples: List[Dict], domain: str) -> pd.DataFrame:\n",
    "        \"\"\"Process samples into DataFrame format\"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            original_text = sample['text']\n",
    "            cleaned_text = self.preprocess_text(original_text)\n",
    "            \n",
    "            if not cleaned_text:\n",
    "                self.stats['skipped_samples'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Create records for each aspect-sentiment pair\n",
    "            for asp_sent in sample['aspect_sentiments']:\n",
    "                aspect = asp_sent['aspect']\n",
    "                sentiment = asp_sent['sentiment']\n",
    "                \n",
    "                # Map sentiment to numeric values\n",
    "                sentiment_numeric = self._map_sentiment(sentiment)\n",
    "                \n",
    "                processed_data.append({\n",
    "                    'sample_id': sample['sample_id'],\n",
    "                    'text': cleaned_text,\n",
    "                    'original_text': original_text,\n",
    "                    'domain': domain,\n",
    "                    'aspect': aspect,\n",
    "                    'sentiment': sentiment,\n",
    "                    'sentiment_numeric': sentiment_numeric\n",
    "                })\n",
    "                \n",
    "                # Update statistics\n",
    "                self._update_stats(domain, aspect, sentiment)\n",
    "                self.stats['processed_samples'] += 1\n",
    "        \n",
    "        return pd.DataFrame(processed_data)\n",
    "    \n",
    "    def _map_sentiment(self, sentiment: str) -> int:\n",
    "        \"\"\"Map sentiment strings to numeric values\"\"\"\n",
    "        sentiment_mapping = {\n",
    "            'positive': 1,\n",
    "            'negative': -1,\n",
    "            'neutral': 0\n",
    "        }\n",
    "        return sentiment_mapping.get(sentiment.lower(), 0)\n",
    "    \n",
    "    def _update_stats(self, domain: str, aspect: str, sentiment: str):\n",
    "        \"\"\"Update processing statistics\"\"\"\n",
    "        # Domain stats\n",
    "        if domain not in self.stats['domains']:\n",
    "            self.stats['domains'][domain] = 0\n",
    "        self.stats['domains'][domain] += 1\n",
    "        \n",
    "        # Aspect stats\n",
    "        if aspect not in self.stats['aspects']:\n",
    "            self.stats['aspects'][aspect] = 0\n",
    "        self.stats['aspects'][aspect] += 1\n",
    "        \n",
    "        # Sentiment stats\n",
    "        if sentiment not in self.stats['sentiments']:\n",
    "            self.stats['sentiments'][sentiment] = 0\n",
    "        self.stats['sentiments'][sentiment] += 1\n",
    "    \n",
    "    def create_colab_compatible_format(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create format compatible with Colab notebooks\"\"\"\n",
    "        # Get unique aspects\n",
    "        all_aspects = sorted(df['aspect'].unique())\n",
    "        \n",
    "        # Group by text to create multi-output format\n",
    "        colab_data = []\n",
    "        \n",
    "        for text_info, group in df.groupby(['sample_id', 'text', 'original_text', 'domain']):\n",
    "            sample_id, text, original_text, domain = text_info\n",
    "            \n",
    "            # Create base record\n",
    "            record = {\n",
    "                'sample_id': sample_id,\n",
    "                'text': text,\n",
    "                'original_text': original_text,\n",
    "                'domain': domain\n",
    "            }\n",
    "            \n",
    "            # Add aspect columns with default neutral (0)\n",
    "            for aspect in all_aspects:\n",
    "                record[aspect] = 0\n",
    "            \n",
    "            # Fill in actual sentiments\n",
    "            for _, row in group.iterrows():\n",
    "                record[row['aspect']] = row['sentiment_numeric']\n",
    "            \n",
    "            colab_data.append(record)\n",
    "        \n",
    "        return pd.DataFrame(colab_data)\n",
    "\n",
    "print(\"VLSP2018 preprocessor loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Raw VLSP2018 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "print(\"=== VIETNAMESE ABSA PREPROCESSING PIPELINE ===\")\n",
    "print(\"Processing VLSP 2018 dataset with Vietnamese-specific steps\\n\")\n",
    "\n",
    "preprocessor = VLSP2018Preprocessor(\n",
    "    apply_vietnamese_steps=True,\n",
    "    apply_tone_normalization=True\n",
    ")\n",
    "\n",
    "# Define input files using downloaded raw data\n",
    "input_files = {\n",
    "    'hotel': {\n",
    "        'train': 'raw_data/VLSP2018-SA-Hotel-train.txt',\n",
    "        'dev': 'raw_data/VLSP2018-SA-Hotel-dev.txt',\n",
    "        'test': 'raw_data/VLSP2018-SA-Hotel-test.txt'\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'train': 'raw_data/VLSP2018-SA-Restaurant-train.txt',\n",
    "        'dev': 'raw_data/VLSP2018-SA-Restaurant-dev.txt',\n",
    "        'test': 'raw_data/VLSP2018-SA-Restaurant-test.txt'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "all_dataframes = {}\n",
    "\n",
    "# Process each domain and split\n",
    "for domain, splits in input_files.items():\n",
    "    print(f\"\\n--- Processing {domain.upper()} domain ---\")\n",
    "    domain_dfs = {}\n",
    "    \n",
    "    for split, file_path in splits.items():\n",
    "        print(f\"Processing {split} set...\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse and process\n",
    "        samples = preprocessor.parse_vlsp_file(file_path)\n",
    "        print(f\"Parsed {len(samples)} samples from {file_path}\")\n",
    "        \n",
    "        # Process into DataFrame\n",
    "        df = preprocessor.process_dataset(samples, domain)\n",
    "        print(f\"Created {len(df)} processed records\")\n",
    "        \n",
    "        # Save individual split\n",
    "        output_path = f'data/{domain}_{split}_processed.csv'\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"Saved to {output_path}\")\n",
    "        \n",
    "        # Create Colab-compatible format\n",
    "        colab_df = preprocessor.create_colab_compatible_format(df)\n",
    "        colab_path = f'data/{domain}_{split}_colab_format.csv'\n",
    "        colab_df.to_csv(colab_path, index=False, encoding='utf-8')\n",
    "        print(f\"Saved Colab format to {colab_path}\")\n",
    "        \n",
    "        domain_dfs[split] = {\n",
    "            'standard': df,\n",
    "            'colab': colab_df\n",
    "        }\n",
    "    \n",
    "    all_dataframes[domain] = domain_dfs\n",
    "\n",
    "print(\"\\n=== PREPROCESSING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display processing statistics\n",
    "print(\"=== PROCESSING STATISTICS ===\")\n",
    "print(f\"Total processed samples: {preprocessor.stats['processed_samples']:,}\")\n",
    "print(f\"Skipped samples: {preprocessor.stats['skipped_samples']:,}\")\n",
    "\n",
    "print(f\"\\nDomain distribution:\")\n",
    "for domain, count in preprocessor.stats['domains'].items():\n",
    "    print(f\"  {domain}: {count:,} records\")\n",
    "\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "for sentiment, count in sorted(preprocessor.stats['sentiments'].items()):\n",
    "    print(f\"  {sentiment}: {count:,} labels\")\n",
    "\n",
    "print(f\"\\nTop 10 aspects:\")\n",
    "sorted_aspects = sorted(preprocessor.stats['aspects'].items(), key=lambda x: x[1], reverse=True)\n",
    "for aspect, count in sorted_aspects[:10]:\n",
    "    print(f\"  {aspect}: {count:,} occurrences\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== SAMPLE PROCESSED DATA ===\")\n",
    "if 'hotel' in all_dataframes and 'train' in all_dataframes['hotel']:\n",
    "    sample_df = all_dataframes['hotel']['train']['standard']\n",
    "    print(f\"\\nHotel training data shape: {sample_df.shape}\")\n",
    "    print(\"\\nFirst 3 records:\")\n",
    "    print(sample_df[['text', 'aspect', 'sentiment']].head(3).to_string(index=False))\n",
    "    \n",
    "    # Show Colab format\n",
    "    colab_sample = all_dataframes['hotel']['train']['colab']\n",
    "    print(f\"\\nColab format shape: {colab_sample.shape}\")\n",
    "    print(f\"Aspect columns: {len([col for col in colab_sample.columns if col not in ['sample_id', 'text', 'original_text', 'domain']])}\")\n",
    "    print(\"\\nFirst record (showing first 5 aspect columns):\")\n",
    "    aspect_cols = [col for col in colab_sample.columns if col not in ['sample_id', 'text', 'original_text', 'domain']][:5]\n",
    "    display_cols = ['text'] + aspect_cols\n",
    "    print(colab_sample[display_cols].head(1).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create usage instructions\n",
    "instructions = \"\"\"\n",
    "=== USAGE INSTRUCTIONS FOR PROCESSED DATA ===\n",
    "\n",
    "The Vietnamese ABSA dataset has been successfully preprocessed and is now ready for use in your evaluation notebooks.\n",
    "\n",
    "FILES CREATED:\n",
    "1. Standard Format (for analysis):\n",
    "   - hotel_train_processed.csv\n",
    "   - hotel_dev_processed.csv\n",
    "   - hotel_test_processed.csv\n",
    "   - restaurant_train_processed.csv\n",
    "   - restaurant_dev_processed.csv\n",
    "   - restaurant_test_processed.csv\n",
    "\n",
    "2. Multi-output Format (for ML models):\n",
    "   - hotel_train_colab_format.csv\n",
    "   - hotel_dev_colab_format.csv\n",
    "   - hotel_test_colab_format.csv\n",
    "   - restaurant_train_colab_format.csv\n",
    "   - restaurant_dev_colab_format.csv\n",
    "   - restaurant_test_colab_format.csv\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Use NLP_Traditional_ML_Fixed.ipynb for traditional ML evaluation\n",
    "2. Use PhoBERT_Fixed_Evaluation.ipynb for PhoBERT evaluation\n",
    "3. Both notebooks will automatically load these processed files\n",
    "\n",
    "PREPROCESSING APPLIED:\n",
    "✓ Vietnamese text cleaning (HTML, emoji, URL removal)\n",
    "✓ Vietnamese tone normalization (VinAI rules)\n",
    "✓ Traditional preprocessing (lowercase, whitespace, short words)\n",
    "✓ Multi-output format for aspect-based classification\n",
    "✓ Proper sentiment mapping (positive: 1, negative: -1, neutral: 0)\n",
    "\n",
    "The data is now ready for academic evaluation and comparison!\n",
    "\"\"\"\n",
    "\n",
    "print(instructions)\n",
    "\n",
    "# Save instructions to file\n",
    "with open('data/preprocessing_instructions.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(instructions)\n",
    "\n",
    "print(\"\\n✓ Instructions saved to: data/preprocessing_instructions.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}