{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqnaXf-eUhsI",
        "outputId": "7e796835-efbf-4149-faea-65329e89cc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def extract_text_and_labels(filepath):\n",
        "    \"\"\"\n",
        "    Extracts (text, labels) pairs from a VLSP2018 ABSA .txt file.\n",
        "    Returns a list of tuples: [(text, [label1, label2, ...]), ...]\n",
        "    \"\"\"\n",
        "    label_pattern = r'\\{([^}]+)\\}'\n",
        "    data = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "\n",
        "    current_text_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line.startswith('{') and '}' in line:\n",
        "            labels = re.findall(label_pattern, line)\n",
        "            full_text = ' '.join(current_text_lines).strip()\n",
        "            data.append((full_text, labels))\n",
        "            current_text_lines = []\n",
        "        else:\n",
        "            current_text_lines.append(line)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_text_label_datasets(base_path):\n",
        "    \"\"\"\n",
        "    Loads datasets with text and associated labels from all domain/split files.\n",
        "    Returns a dictionary:\n",
        "    {\n",
        "        'restaurant': {'train': [...], 'dev': [...], 'test': [...]},\n",
        "        'hotel': {'train': [...], 'dev': [...], 'test': [...]}\n",
        "    }\n",
        "    \"\"\"\n",
        "    dataset = {'restaurant': {}, 'hotel': {}}\n",
        "    for domain in ['restaurant', 'hotel']:\n",
        "        for split in ['train', 'dev', 'test']:\n",
        "            filename = f\"VLSP2018-SA-{domain.capitalize()}-{split}.txt\"\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                dataset[domain][split] = extract_text_and_labels(filepath)\n",
        "            else:\n",
        "                print(f\"Missing file: {filepath}\")\n",
        "    return dataset\n",
        "\n",
        "base_folder = \"/content/drive/MyDrive/NLP Project\"\n",
        "full_data = load_text_label_datasets(base_folder)\n",
        "\n",
        "# Print example output\n",
        "for domain in ['hotel', 'restaurant']:\n",
        "    for split in ['train', 'dev', 'test']:\n",
        "        if full_data[domain].get(split):\n",
        "            text, labels = full_data[domain][split][0]\n",
        "            print(f\"\\n[{domain.capitalize()} {split.capitalize()}]\")\n",
        "            print(\"Text:\", text[:300] + (\"...\" if len(text) > 300 else \"\"))\n",
        "            print(\"Labels:\", labels)\n",
        "        else:\n",
        "            print(f\"\\n[{domain.capitalize()} {split.capitalize()}] No data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9xLFyKkK98a",
        "outputId": "70b18785-f0cc-4ea6-e3fb-4f9c78d440a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Hotel Train]\n",
            "Text: ﻿#1 Rộng rãi KS mới nhưng rất vắng. Các dịch vụ chất lượng chưa cao và thiếu.\n",
            "Labels: ['HOTEL#DESIGN&FEATURES, positive', 'HOTEL#GENERAL, negative']\n",
            "\n",
            "[Hotel Dev]\n",
            "Text: ﻿#1 Chưa có thang máy. Chưa chấp nhận thanh toán bằng thẻ. Địa điểm dễ tìm, bày trí bằng tre nứa rất mát mẻ, bạn lễ tân nhiệt tình, niềm nở, thân thiện, tốt bụng cực kỳ. Tôi đặt phòng 1 giường đôi nhưng biết tôi đi cùng 2 con nhỏ nên ks đã chủ động chuẩn bị thêm 1 chiếc giường tầng cho 2 bé. Phòng r...\n",
            "Labels: ['FACILITIES#DESIGN&FEATURES, negative', 'SERVICE#GENERAL, positive', 'LOCATION#GENERAL, positive', 'HOTEL#DESIGN&FEATURES, positive', 'HOTEL#COMFORT, positive', 'ROOMS#DESIGN&FEATURES, positive', 'ROOM_AMENITIES#QUALITY, positive', 'ROOM_AMENITIES#CLEANLINESS, positive', 'HOTEL#GENERAL, positive']\n",
            "\n",
            "[Hotel Test]\n",
            "Text: ﻿#1 Ga giường không sạch, nhân viên quên dọn phòng một ngày.\n",
            "Labels: ['ROOM_AMENITIES#CLEANLINESS, negative', 'SERVICE#GENERAL, negative']\n",
            "\n",
            "[Restaurant Train]\n",
            "Text: ﻿#1 _ Ảnh chụp từ hôm qua, đi chơi với gia đình và 1 nhà họ hàng đang sống tại Sài Gòn. _ Hôm qua đi ăn trưa muộn, ai cũng đói hết nên lúc có đồ ăn là nhào vô ăn liền, bởi vậy mới quên chụp các phần gọi thêm với nước mắm, chỉ chụp món chính thôi! _ Đói quá nên không biết đánh giá đồ ăn kiểu gì luôn ...\n",
            "Labels: ['FOOD#STYLE&OPTIONS, neutral', 'FOOD#QUALITY, neutral']\n",
            "\n",
            "[Restaurant Dev]\n",
            "Text: ﻿#1 - 60k/1con- Tu hài to, siêu béo siêu ngon, nướng mỡ hành thơm phức, béo ngậy- Đĩa tu hài đem ra nóng hổi, gắp 1 miếng vào miệng kích thích vi giác kinh khủng- 1 con to đến mức họ phải cắt ra làm đôi, ăn nửa con đầy ý miệng luôn ý. Mà giá đó cho 1 con tu hài ngon như vậy là quá rẻ!\n",
            "Labels: ['FOOD#PRICES, positive', 'FOOD#QUALITY, positive']\n",
            "\n",
            "[Restaurant Test]\n",
            "Text: ﻿#1 Đây là 1 trong những quán mà mình thích vì vị trà đậm và thơm cũng như mùi vị đặc trưng hơn hẳn những quán khác nè  Trà sữa trân châu sợi - 46k Trà sữa pha khá ngon, vị trà chát và mùi hương khá rõ, không quá ngọt, rất đúng với gu mình  Trà đào - 45k Vị trà đào ở đây cũng đặc biệt hơn hẳn những ...\n",
            "Labels: ['RESTAURANT#GENERAL, positive', 'DRINKS#QUALITY, positive', 'DRINKS#PRICES, neutral', 'DRINKS#STYLE&OPTIONS, neutral']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "!pip install pyvi\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "# -------------------- Normalization Functions --------------------\n",
        "\n",
        "def normalize_money(sent):\n",
        "    return re.sub(r'[0-9]+[.,0-9]*[kmb]', 'giá', sent)\n",
        "\n",
        "def normalize_hastag(sent):\n",
        "    return re.sub(r'#(\\w+)', r'\\1', sent)\n",
        "\n",
        "def normalize_website(sent):\n",
        "    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F]{2}))+', 'website', sent)\n",
        "    return re.sub(r'\\w+(\\.(com|vn|me))+((/+([\\w\\.\\-]+)?)+)?', 'website', result)\n",
        "\n",
        "def nomalize_emoji(sent):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\u2600-\\u26FF\\u2700-\\u27BF\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', sent)\n",
        "\n",
        "def normalize_elongate(sent):\n",
        "    pattern = r'(.)\\1{1,}'\n",
        "    result = sent\n",
        "    while re.search(pattern, result):\n",
        "        repeat_char = re.search(pattern, result)\n",
        "        result = result.replace(repeat_char.group(0), repeat_char.group(1))\n",
        "    return result\n",
        "\n",
        "def remove_number(sent):\n",
        "    return re.sub(r'[0-9]+', '', sent)\n",
        "\n",
        "def normalize_acronyms(sent):\n",
        "    replace_list = {\n",
        "        'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ', 'okey': ' ok ',\n",
        "        'authentic': ' chuẩn chính hãng ', 'fake': ' giả mạo ', 'shop': ' cửa hàng ',\n",
        "        'gud': ' tốt ', 'wel done': ' tốt ', 'good': ' tốt ', 'bad': ' tệ ',\n",
        "        'huhu': ' tiêu cực ', 'haha': ' tích cực ', 'cute': ' dễ thương ', 'lol': ' tiêu cực ',\n",
        "        'thanks': ' cám ơn ', 'thks': ' cám ơn ', 'tks': ' cám ơn ',\n",
        "        'ship': ' giao hàng ', 'delivery': ' giao hàng ', 'rep': ' trả lời ',\n",
        "        'fb': ' facebook ', 'face': ' facebook ', 'sp': ' sản phẩm ',\n",
        "        'nt': ' nhắn tin ', 'tl': ' trả lời ', 'dt': ' điện thoại ', 'sd': ' sử dụng ',\n",
        "        'bt': ' bình thường ', 'perfect': ' rất tốt ', 'nice': ' tốt ', 'fresh': ' tươi ',\n",
        "        'iu': ' yêu ', 'dep': ' đẹp ', 'xau': ' xấu ', 'delicious': ' ngon ',\n",
        "        'fback': ' feedback ', 'fedback': ' feedback '\n",
        "    }\n",
        "    for k, v in replace_list.items():\n",
        "        sent = sent.replace(k, v)\n",
        "    return sent\n",
        "\n",
        "def normalize(sent):\n",
        "    sent = normalize_money(sent)\n",
        "    sent = normalize_hastag(sent)\n",
        "    sent = normalize_website(sent)\n",
        "    sent = nomalize_emoji(sent)\n",
        "    sent = normalize_elongate(sent)\n",
        "    sent = normalize_acronyms(sent)\n",
        "    sent = remove_number(sent)\n",
        "    sent = sent.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
        "    sent = re.sub(r'\\s+', ' ', sent).strip()\n",
        "    return sent\n",
        "\n",
        "def tokenize(sent):\n",
        "    return ViTokenizer.tokenize(sent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ela9fJI2Pw7a",
        "outputId": "445b1b45-3e34-42e6-b44b-7b4071bf3c42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Review + Label Reader --------------------\n",
        "\n",
        "def extract_clean_tokenized_text_and_labels(filepath):\n",
        "    \"\"\"\n",
        "    Extracts cleaned and tokenized (text, labels) pairs from VLSP2018 file.\n",
        "    \"\"\"\n",
        "    label_pattern = r'\\{([^}]+)\\}'\n",
        "    data = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "\n",
        "    current_text_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line.startswith('{') and '}' in line:\n",
        "            labels = re.findall(label_pattern, line)\n",
        "            raw_text = ' '.join(current_text_lines).strip()\n",
        "            cleaned = normalize(raw_text)\n",
        "            tokenized = tokenize(cleaned)\n",
        "            data.append((tokenized, labels))\n",
        "            current_text_lines = []\n",
        "        else:\n",
        "            current_text_lines.append(line)\n",
        "\n",
        "    return data\n",
        "\n",
        "def load_text_label_datasets(base_path):\n",
        "    dataset = {'restaurant': {}, 'hotel': {}}\n",
        "    for domain in ['restaurant', 'hotel']:\n",
        "        for split in ['train', 'dev', 'test']:\n",
        "            filename = f\"VLSP2018-SA-{domain.capitalize()}-{split}.txt\"\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                dataset[domain][split] = extract_clean_tokenized_text_and_labels(filepath)\n",
        "            else:\n",
        "                print(f\"Missing file: {filepath}\")\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "acMPj4xzP215"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = \"/content/drive/MyDrive/NLP Project\"\n",
        "full_data = load_text_label_datasets(base_folder)\n",
        "\n",
        "# Example: print first cleaned + tokenized entry\n",
        "text, labels = full_data['restaurant']['dev'][0]\n",
        "print(\"Tokenized Text:\", text)\n",
        "print(\"Labels:\", labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAL2IGr2QCOp",
        "outputId": "e1918895-a10a-4828-f547-b1574006f5f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Text: ﻿ giá con Tu hài to siêu béo siêu ngon nướng mỡ hành thơm_phức béo ngậy Đĩa tu hài đem ra nóng_hổi gắp miếng vào miệng kích_thích vi giác kinh_khủng con to đến mức họ phải cắt ra làm đôi ăn nửa con đầy ý miệng luôn ý Mà giá đó cho con tu hài ngon như_vậy là quá rẻ\n",
            "Labels: ['FOOD#PRICES, positive', 'FOOD#QUALITY, positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def dataset_to_dataframe(dataset):\n",
        "    rows = []\n",
        "    for domain, splits in dataset.items():\n",
        "        for split, entries in splits.items():\n",
        "            for text, labels in entries:\n",
        "                aspects = []\n",
        "                opinions = []\n",
        "                for label in labels:\n",
        "                    if ',' in label:\n",
        "                        aspect, opinion = label.rsplit(',', 1)\n",
        "                        aspects.append(aspect.strip())\n",
        "                        opinions.append(opinion.strip())\n",
        "\n",
        "                rows.append({\n",
        "                    'domain': domain,\n",
        "                    'split': split,\n",
        "                    'text': text,\n",
        "                    'labels': labels,\n",
        "                    'aspects': aspects,\n",
        "                    'opinions': opinions\n",
        "                })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = dataset_to_dataframe(full_data)\n",
        "\n",
        "# Show the first few rows\n",
        "df.head(5)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "dj87LkEfQm7o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "SeX7ioYcWnyD"
      }
    }
  ]
}